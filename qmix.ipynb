{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QMix applied to pommerman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Import error NSDE! You will not be able to render --> Cannot connect to \"None\"\n"
     ]
    }
   ],
   "source": [
    "import pommerman\n",
    "from pommerman import agents\n",
    "from pommerman import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(obs):\n",
    "    \"\"\"Returns a tensor of size 11x11x18\"\"\"\n",
    "    # TODO: history of n moves?\n",
    "    board = obs['board']\n",
    "\n",
    "    # convert board items into bitmaps\n",
    "    maps = [board == i for i in range(10)]\n",
    "    maps.append(obs['bomb_blast_strength'])\n",
    "    maps.append(obs['bomb_life'])\n",
    "\n",
    "    # duplicate ammo, blast_strength and can_kick over entire map\n",
    "    maps.append(np.full(board.shape, obs['ammo']))\n",
    "    maps.append(np.full(board.shape, obs['blast_strength']))\n",
    "    maps.append(np.full(board.shape, obs['can_kick']))\n",
    "\n",
    "    # add my position as bitmap\n",
    "    position = np.zeros(board.shape)\n",
    "    position[obs['position']] = 1\n",
    "    maps.append(position)\n",
    "\n",
    "    # add teammate\n",
    "    if obs['teammate'] is not None:\n",
    "        maps.append(board == obs['teammate'].value)\n",
    "    else:\n",
    "        maps.append(np.zeros(board.shape))\n",
    "\n",
    "    # add enemies\n",
    "    enemies = [board == e.value for e in obs['enemies']]\n",
    "    maps.append(np.any(enemies, axis=0))\n",
    "\n",
    "    out = np.stack(maps, axis=2) \n",
    "    # transpose to CxHxW\n",
    "    return out.transpose((2, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'actions', 'next_state', 'rewards', 'done'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def push_episode(self, episode):\n",
    "        for transition in episode:\n",
    "            self.push(*transition)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, render=False):\n",
    "    state, done = env.reset(), False\n",
    "    episode = []\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        actions = env.act(state)\n",
    "        next_state, rewards, done, info = env.step(actions)\n",
    "        episode.append(Transition(state, actions, next_state, rewards, done))\n",
    "        state = next_state\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_stats(env, episode):\n",
    "    \"\"\"Collects statistics from episode\n",
    "    For now, only about actions taken\"\"\"\n",
    "    \n",
    "    from collections import Counter\n",
    "    action_counter = {}\n",
    "    for idx in range(len(env._agents)):\n",
    "        action_counter[idx] = Counter()\n",
    "    for transition in episode:\n",
    "        for idx, action in enumerate(transition.actions):\n",
    "            action_counter[idx][list(constants.Action)[action].name] += 1\n",
    "    return {'actions': action_counter}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/starry-sky6688/StarCraft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMixAgent(agents.BaseAgent):\n",
    "    def __init__(self, agent_idx):\n",
    "        super().__init__()\n",
    "        self.index = agent_idx\n",
    "        self.epsilon = 1.0\n",
    "        self.model  = QMixModel().to(args.device)\n",
    "        self.target = QMixModel().to(args.device)\n",
    "        self.sync_models()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(),\n",
    "                                    lr=args.lr)\n",
    "        self.mode = 'train' # 'train' or 'eval'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'QMix{self.index}'\n",
    "    \n",
    "    __str__ = __repr__\n",
    "    \n",
    "    def set_mode(self, mode):\n",
    "        assert mode in ['train', 'eval'], f\"Mode {mode} not allowed\"\n",
    "        self.mode = mode\n",
    "    \n",
    "    def sync_models(self):\n",
    "        self.target.load_state_dict(self.model.state_dict())\n",
    "        \n",
    "    def update(self, batch):\n",
    "        self.mode = 'train'\n",
    "        states, actions, next_states, rewards, dones = list(zip(*batch))\n",
    "        obs      = [state[self.index] for state in states]\n",
    "        actions  = [action[self.index] for action in actions]\n",
    "        next_obs = [state[self.index] for state in next_states]\n",
    "        rewards  = torch.tensor([reward[self.index] for reward in rewards]).float().to(args.device)\n",
    "        dones    = torch.tensor(dones).float().to(args.device)\n",
    "        \n",
    "        q_vals  = self.model([featurize(o) for o in obs])\n",
    "        qa_vals = q_vals[range(len(obs)), actions]\n",
    "        \n",
    "        q_vals  = self.target([featurize(o) for o in next_obs])\n",
    "        q_vals_max, _ = torch.max(q_vals, dim=1)\n",
    "        td_error = rewards + (1-dones) * args.gamma * q_vals_max - qa_vals\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        loss = torch.mean(td_error**2)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def _update_epsilon(self):\n",
    "        self.epsilon *= args.eps_decay \n",
    "        self.epsilon = max(args.eps_min, self.epsilon)\n",
    "        \n",
    "    def act(self, obs, action_space):\n",
    "        if self.mode == 'train':\n",
    "            self._update_epsilon()\n",
    "            if np.random.random() < self.epsilon:\n",
    "                return np.random.choice(constants.Action).value\n",
    "            else:\n",
    "                Qs = self.model([featurize(obs)])\n",
    "                return torch.argmax(Qs).item()\n",
    "        elif self.mode == 'eval':\n",
    "            Qs = self.model([featurize(obs)])\n",
    "            return torch.argmax(Qs).item()\n",
    "        else:\n",
    "            raise ValueError(f'Invalid mode: {self.mode}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMixModel(nn.Module):\n",
    "    def __init__(self, h=11, w=11, c=18, outputs=len(constants.Action)):\n",
    "        super().__init__()\n",
    "        # input is batch of tensors of size 11x11x18\n",
    "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=64,\n",
    "                               kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128,\n",
    "                               kernel_size=5, stride=1)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size=5, stride=1):\n",
    "            return (size - (kernel_size - 1) -1) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(w))\n",
    "        convh = conv2d_size_out(conv2d_size_out(h))\n",
    "        linear_input_size = convw * convh * 128\n",
    "        \n",
    "        self.fc  = nn.Linear(linear_input_size, linear_input_size)\n",
    "        self.out = nn.Linear(linear_input_size, outputs)\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        if isinstance(obs, list):\n",
    "            x = torch.from_numpy(np.array(obs)).float().to(args.device)\n",
    "        else:\n",
    "            x = torch.from_numpy(obs).float().to(args.device)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.out(x).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, training_agents, n_steps=10000): \n",
    "    buffer = ReplayMemory(capacity=args.buffer_size)\n",
    "    state, done = env.reset(), False\n",
    "    running_loss = None\n",
    "    for step_idx in range(n_steps):\n",
    "        actions = env.act(state)\n",
    "        next_state, rewards, done, info = env.step(actions)\n",
    "        buffer.push(state, actions, next_state, rewards, done)\n",
    "        if len(buffer) < args.min_train_size:\n",
    "            continue\n",
    "        batch = buffer.sample(args.batch_size)\n",
    "        for agent in training_agents:\n",
    "            loss = agent.update(batch)\n",
    "            running_loss = loss if running_loss is None else args.alpha*running_loss + (1-args.alpha)*loss\n",
    "            if args.verbose and step_idx % args.print_interval == 0:\n",
    "                print(f\"Step {step_idx:3d} - Agent {agent}: loss = {running_loss:.5f}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(env, training_agents, n_episodes=10):\n",
    "    \"Generates n_episodes episodes and returns average final reward for all training agents\"\n",
    "    for agent in training_agents:\n",
    "        agent.set_mode('eval')\n",
    "    rewards = [generate_episode(env)[-1].rewards for _ in range(n_episodes)]\n",
    "    agent_reward = {}\n",
    "    for agent in training_agents:\n",
    "        agent_reward[agent] = sum([reward[agent.index] for reward in rewards])/n_episodes\n",
    "    return agent_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner():\n",
    "    training_agents = [QMixAgent(0)]\n",
    "    other_agents    = [agents.SimpleAgent()]\n",
    "    agent_list = training_agents + other_agents\n",
    "    env = pommerman.make('PommeFFACompetition-v0', agent_list)\n",
    "    for epoch in range(args.n_epochs):\n",
    "        train(env, training_agents, n_steps=args.n_steps)\n",
    "        rewards = eval(env, training_agents)\n",
    "        print(f\"Epoch {epoch:3d} - Wins = {rewards}\")\n",
    "        stats   = episode_stats(env, generate_episode(env, render=False))\n",
    "        for agent in training_agents:\n",
    "            print('\\t', agent, ' : ', stats['actions'][agent.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "class args:\n",
    "    buffer_size    = int(10e4)\n",
    "    min_train_size = 100\n",
    "    batch_size     = 64\n",
    "    gamma          = 0.9\n",
    "    lr             = 0.001\n",
    "    alpha          = 0.9\n",
    "    print_interval = 20\n",
    "    n_epochs       = 50\n",
    "    n_steps        = 200\n",
    "    verbose        = False\n",
    "    eps_decay      = 1 - 10e-9\n",
    "    eps_min        = 0.05\n",
    "    device         = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMix:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.n_actions = args.n_actions\n",
    "        self.n_agents  = args.n_agents\n",
    "        self.state_shape = args.state_shape\n",
    "        self.obs_shape = args.obs_shape\n",
    "        input_shape  = self.obs_shape\n",
    "        #input_shape += self.n_actions # add last action to agent network input\n",
    "        #input_shape += self.n_agents  # reuse agent network for all agents (-> weight sharing)\n",
    "        \n",
    "        self.eval_rnn = RNN(input_shape, args)   # the agent network that produces Q_a(.)\n",
    "        self.target_rnn = RNN(input_shape, args)\n",
    "        self.eval_qmix_net = QMixNet(args)       # the mixer netwok Qtot = f(Q1, ..., Qn, state)\n",
    "        self.target_qmix_net = QMixNet(args)\n",
    "        \n",
    "        # copy weigths from eval to target networks\n",
    "        self.target_rnn.load_state_dict(self.eval_rnn.state_dict())\n",
    "        self.target_qmix_net.load_state_dict(self.eval_qmix_net.state_dict())\n",
    "        \n",
    "        self.eval_parameters = list(self.eval_qmix_net.parameters()) + \\\n",
    "                               list(self.eval_rnn.parameters())\n",
    "        self.optimizer = torch.optim.RMSprop(self.eval_parameters, lr=args.lr)\n",
    "        \n",
    "        self.eval_hidden = None\n",
    "        self.target_hidden = None\n",
    "        print('Initialized QMix')\n",
    "    \n",
    "    def learn(self, max_episode_len, train_step, epsilon=None):\n",
    "        \"\"\"\n",
    "        In learning, the extracted data is four-dimensional, and the four dimensions\n",
    "        are:\n",
    "            1-which is the first episode\n",
    "            2-which is the transition of the episode\n",
    "            3—The data of which agent \n",
    "            4—Specific obs dimension. \n",
    "        Because when selecting an action, not only the current inputs need to be input,\n",
    "        but also hidden_state is input to the neural network.\n",
    "        hidden_state is related to previous experience, so you cannot randomly select\n",
    "        experience for learning. So here we extract multiple episodes at once, and then\n",
    "        give them to the neural network at a time\n",
    "        Transition in the same position of each episode\n",
    "        \"\"\"\n",
    "        episode_num = batch['o'].shape[0] # shape of 'o': (number_of_episodes x episode_limit x n_agents x obs_shape)\n",
    "        self.init_hidden(episode_num)\n",
    "        for key in batch.keys():\n",
    "            batch[key] = torch.tensor(batch[key], dtype=torch.float32)\n",
    "        s, s_next, u, r, avail_u, avail_u_next, terminated = batch['s'], batch['s_next'], \\\n",
    "                                                             batch['u'], batch['r'], \\\n",
    "                                                             batch['avail_u'], batch['avail_u_next'], \\\n",
    "                                                             batch['terminated']\n",
    "        mask = 1 - batch[\"padded\"] # padded = 1 if added zeros to get to episode limit\n",
    "        \n",
    "        # 1. Agent networks\n",
    "        # Get the Q value corresponding to each agent, the dimension is (number of episodes, max_episode_len, n_agents, n_actions)\n",
    "        q_evals, q_targets = self.get_q_values(batch, max_episode_len)\n",
    "        # select q_vals for u = actions taken + remove unneeded dimension\n",
    "        q_evals = torch.gather(q_evals, dim=3, index=u).squeeze(3)\n",
    "        \n",
    "        # get target_q by maximizing - first, set q[unavail_action] = 0\n",
    "        q_targets[avail_u_next == 0.0] = -99999\n",
    "        q_targets = q_targets.max(dim=3)[0]\n",
    "        \n",
    "        # 2. QMixer to obtain Qtot\n",
    "        q_total_eval   = self.eval_qmix_net(q_evals, s)\n",
    "        q_total_target = self.target_qmix_net(q_targets, s_next)\n",
    "        \n",
    "        # 3. Compute total loss\n",
    "        targets  = r + self.args.gamma * q_total_target * (1 - terminated)\n",
    "        td_error = (q_total_eval - targets.detach())\n",
    "        masked_td_error = mask * td_error # set td_error to zero for filled-up experience\n",
    "        \n",
    "        loss =(masked_td_error ** 2).sum() / mask.sum()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(self.eval_parameters, self.args.grad_norm_clip)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if train_step > 0 and train_step % self.args.target_update_cycle == 0:\n",
    "            self.target_rnn.load_state_dict(self.eval_rnn.state_dict())\n",
    "            self.target_qmix_net.load_state_dict(self.eval_qmix_net.state_dict())\n",
    "    \n",
    "    def _get_inputs(self, batch, transition_idx):\n",
    "        \"\"\"Get all obs, next_obs and actions (as onehot vector) at\n",
    "        transition_idx (over all episodes in batch)\"\"\"\n",
    "        obs, obs_next, u_onehot = batch['o'][:, transition_idx], \\\n",
    "                                  batch['o_next'][:, transition_idx], \\\n",
    "                                  batch['u_onehot'][:]\n",
    "        episode_num = obs.shape[0]\n",
    "        inputs, inputs_next = [], []\n",
    "        inputs.append(obs)\n",
    "        inputs_next.append(obs_next)\n",
    "        if self.args.last_action:   # whether to use the last action to choose action\n",
    "            if transition_idx == 0:  # for first experience, previous action is zero vector\n",
    "                inputs.append(torch.zeros_like(u_onehot[:, transition_idx]))\n",
    "            else:\n",
    "                inputs.append(u_onehot[:, transition_idx - 1])\n",
    "            inputs_next.append(u_onehot[:, transition_idx])\n",
    "        if self.args.reuse_network: # weight sharing: whether to use one network for all agents\n",
    "            pass\n",
    "        \n",
    "        # transform inputs to episode_num x n_agents x ....\n",
    "        inputs = torch.cat([x.reshape(episode_num * self.args.n_agents, -1) for x in inputs], dim=1)\n",
    "        inputs_next = torch.cat([x.reshape(episode_num * self.args.n_agents, -1) for x in inputs_next], dim=1)\n",
    "        return inputs, inputs_next\n",
    "        \n",
    "        \n",
    "    def get_q_values(self, batch, max_episode_len):\n",
    "        episode_num = batch['o'].shape[0]\n",
    "        q_evals, q_targets = [], []\n",
    "        for transition_idx in range(max_episode_len):\n",
    "            inputs, inputs_next = self._get_inputs(batch, transition_idx)\n",
    "            q_eval, self.eval_hidden     = self.eval_rnn(inputs, self.eval_hidden)\n",
    "            q_target, self.target_hidden = self.target_rnn(inputs_next, self.target_hidden)\n",
    "            # reshape q_eval back to (max_episode_len, n_agents, n_actions)\n",
    "            q_eval   = q_eval.view(episode_num, self.n_agents, -1)\n",
    "            q_target = q_target.view(episode_num, self.n_agents, -1)\n",
    "            q_evals.append(q_eval)\n",
    "            q_targets.append(q_target)\n",
    "        # The obtained q_eval and q_target are a list, the list contains max_episode_len arrays,\n",
    "        # the dimension of the array is (number of episodes, n_agents, n_actions)\n",
    "        # Convert the list into an array of \n",
    "        # (number of episodes, max_episode_len, n_agents, n_actions)\n",
    "        q_evals   = torch.stack(q_evals, dim=1)\n",
    "        q_targets = torch.stack(q_targets, dim=1)\n",
    "        return q_evals, q_targets\n",
    "    \n",
    "    def init_hidden(self, episode_num):\n",
    "        self.eval_hidden = torch.zeros((episode_num, self.n_agents, self.args.rnn_hidden_dim))\n",
    "        self.target_hidden = torch.zeros((episode_num, self.n_agents, self.args.rnn_hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    # Because all the agents share the same network, input_shape=obs_shape+n_actions+n_agents\n",
    "    def __init__(self, input_shape, args):\n",
    "        super(RNN, self).__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_shape, args.rnn_hidden_dim)\n",
    "        self.rnn = nn.GRUCell(args.rnn_hidden_dim, args.rnn_hidden_dim)\n",
    "        self.fc2 = nn.Linear(args.rnn_hidden_dim, args.n_actions)\n",
    "    \n",
    "    def forward(self, obs, hidden_state):\n",
    "        x = F.relu(self.fc1(obs))\n",
    "        h_in = hidden_state.reshape(-1, self.args.rnn_hidden_dim)\n",
    "        h = self.rnn(x, h_in)\n",
    "        q = self.fc2(h)\n",
    "        return q, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMixNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(QMixNet, self).__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        # hyper_w1 has to be a matrix, and pytorch nn.Linear only outputs a vector,\n",
    "        # first output vector of length n_row*n_cols and convert it to matrix\n",
    "        self.hyper_w1 = nn.Linear(args.state_shape, args.n_agents * args.qmix_hidden_dim)\n",
    "        self.hyper_w2 = nn.Linear(args.state_shape, args.qmix_hidden_dim)\n",
    "        self.hyper_b1 = nn.Linear(args.state_shape, args.qmix_hidden)\n",
    "        self.hyper_b2 = nn.Sequential(nn.Linear(args.state_shape, args.qmix_hidden_dim),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(args.qmix_hidden_dim, 1))\n",
    "    \n",
    "    def forward(self, q_values, states):\n",
    "        episode_num = q_values.size(0)\n",
    "        q_values = q_values.view(-1, 1, self.args.n_agents) # (episode_num x max_episode_len, 1, n_agents)\n",
    "        states = states.reshape(-1, self.args.state_shape)  # (episode_num x max_episode-len, state_shpe)\n",
    "        \n",
    "        w1 = torch.abs(self.hyper_w1(states))                           # (1920, 160)\n",
    "        w1 = w1.view(-1, self.args.n_agents, self.args.qmix_hidden_dim) # (1920, 5, 32)\n",
    "        \n",
    "        b1 = self.hyper_b1(states)                      # (1920, 32)\n",
    "        b1 = b1.view(-1, 1, self.args.qmix_hidden_dim)  # (1920, 1, 32)\n",
    "        \n",
    "        hidden = F.elu(torch.bmm(q_values, w1) + b1)  # (1920, 1, 32)\n",
    "        \n",
    "        w2 = torch.abs(self.hyper_w2(states))  # (1920, 32)\n",
    "        b2 = self.hyper_b2(states)  # (1920, 1)\n",
    "\n",
    "        w2 = w2.view(-1, self.args.qmix_hidden_dim, 1)  # (1920, 32, 1)\n",
    "        b2 = b2.view(-1, 1, 1)  # (1920, 1， 1)\n",
    "\n",
    "        q_total = torch.bmm(hidden, w2) + b2  # (1920, 1, 1)\n",
    "        q_total = q_total.view(episode_num, -1, 1)  # (32, 60, 1)\n",
    "        return q_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agents:\n",
    "    def __init__(self, args):\n",
    "        self.n_actions = args.n_actions\n",
    "        self.n_agents  = args.n_agents\n",
    "        self.state_shape = args.state_shape\n",
    "        self.obs_shape = args.obs_shape\n",
    "        self.policy = QMix(args)\n",
    "        print('Initialized Agents')\n",
    "    \n",
    "    def choose_action(self, obs, last_action, agent_num, avail_actions, epsilon, evaluate=False):\n",
    "        inputs = obs.copy()\n",
    "        avail_actions_ind = np.nonzero(avail_actions)[0]  # index of actions that can be chosen\n",
    "                                                          # np.zeros returns the indices of the\n",
    "                                                          # elements that are non-zero.\n",
    "        \n",
    "        # transform agent_num to onehot vector\n",
    "        agent_id = np.zeros(self.n_agents)\n",
    "        agent_id[agent_num] = 1\n",
    "        \n",
    "        inputs = np.hstack((inputs, last_action)) # add last action to RNN inputs\n",
    "        inputs = np.hstack((inputs, agent_id))    # add agent id to RNN inputs\n",
    "        \n",
    "        # pick hidden state corresponding to current agent\n",
    "        hidden_state = self.policy.eval_hidden[:, agent_num, :] \n",
    "        \n",
    "        # add a first dimension (batchsize=1) to inputs tensor (from (42,) to (1,42))) #TODO check tensor\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0).to(args.device)\n",
    "        avail_actions = torch.tensor(avail_actions, dtype=torch.float32).unsqueeze(0).to(args.device)\n",
    "        hidden_state = hidden_state.to(args.device)\n",
    "        \n",
    "        # get q value\n",
    "        q_value, self.policy.eval_hidden[:, agent_num, :] = self.policy.eval_rnn(inputs, hidden_state)\n",
    "        \n",
    "        # choose action from q value\n",
    "        q_value[avail_actions == 0.0] = -float('inf')\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = np.random.choice(avail_actions_ind)\n",
    "        else:\n",
    "            action = torch.argmax(q_value)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def train(self, batch, train_step):\n",
    "        # different episodes have different lengths, so we need to get max length of the batch\n",
    "        max_episode_len = self._get_max_episode_len(batch)\n",
    "        for key in batch.keys():\n",
    "            if key != 'z': # TODO: what is 'z'? -> MAVEN\n",
    "                batch[key] = batch[key][:, :max_episode_len]\n",
    "        self.policy.learn(batch, max_episode_len, train_step)\n",
    "        if train_step > 0 and train_step % self.args.save_cycle == 0:\n",
    "            self.policy.save_model(train_step)\n",
    "    \n",
    "    def _get_max_episode_len(self, batch):\n",
    "        terminated = batch['terminated']\n",
    "        episode_num = terminated_shape[0] # number of episode in batch\n",
    "        max_episode_len = 0\n",
    "        for episode_idx in range(episode_num):\n",
    "            for transition_idx in range(self.args.episode_limit):\n",
    "                if terminated[episode_idx, transition_idx, 0] == 1:\n",
    "                    if transition_idx + 1 >= max_episode_len:\n",
    "                        max_episode_len = transition_idx + 1\n",
    "                    break\n",
    "        return max_episode_len\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_state(env):\n",
    "    \"\"\"Returns a tensor of size n_agentsx11x11x18\"\"\"\n",
    "    outs = []\n",
    "    for obs in env.get_observations():\n",
    "        outs.append(featurize(obs))\n",
    "        \n",
    "    # convert board items into bitmaps\n",
    "    maps = [board == i for i in range(10)] # returns list of 10 arrays, for each type 0..9\n",
    "\n",
    "    outs = np.stack(outs, axis=0)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_agents = [QMixAgent(0)]\n",
    "other_agents    = [agents.SimpleAgent()]\n",
    "agent_list = training_agents + other_agents\n",
    "env = pommerman.make('PommeFFACompetition-v0', agent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutWorker:\n",
    "    def __init__(self, env, agents, args):\n",
    "        self.env = env\n",
    "        self.agents   = agents\n",
    "        self.n_agents = len(agents)\n",
    "        \n",
    "        self.args = args\n",
    "        self.epsilon = args.epsilon\n",
    "        self.episode_limit = args.episode_limit\n",
    "        \n",
    "        print('Init RolloutWorker')\n",
    "        \n",
    "    def generate_episode(self, episode_num=None, evaluate=False):\n",
    "        o, u, r, s, avail_u, u_onehot, terminate, padded = [], [], [], [], [], [], [], []\n",
    "        # padded = list of zeros and ones; one if corresponding values in other lest are 'padded', \n",
    "        #  i.e. added to have episode_limit and uniform output size\n",
    "        \n",
    "        self.env.reset()\n",
    "        \n",
    "        terminated = False\n",
    "        step = 0\n",
    "        last_action = np.zeros((self.args.n_agents, self.args.n_actions))\n",
    "        self.agents.policy.init_hidden(1)\n",
    "        \n",
    "        epsilon = 0 if evaluate else self.epsilon\n",
    "        \n",
    "        \n",
    "        while not terminated and step < self.episode_limit:\n",
    "            obs = self.env.get_observations()\n",
    "            state = self.env.get_state()\n",
    "            actions, avail_actions, actions_onehot = [], [], []\n",
    "            for agent_id in range(self.n_agents):\n",
    "                avail_action = self.env.get_avail_agent_actions(agent_id)\n",
    "                action = self.agents.choose_action(obs[agent_id], last_action[agent_id], agent_id,\n",
    "                                                   avail_action, epsilon, evaluate)\n",
    "                # generate onehot vector of th action\n",
    "                action_onehot = np.zeros(self.args.n_actions)\n",
    "                action_onehot[action] = 1\n",
    "                \n",
    "                actions.append(action)\n",
    "                actions_onehot.append(action_onehot)\n",
    "                avail_actions.append(avail_action)\n",
    "                last_action[agent_id] = action_onehot\n",
    "            \n",
    "            reward, terminated, info = self.env.step(actions)\n",
    "            win_tag = True if terminated and 'battle_won' in info and info['battle_won'] else False\n",
    "            \n",
    "            # store all results \n",
    "            o.append(obs)\n",
    "            s.append(state)\n",
    "            u.append(np.reshape(actions, [self.n_agents, 1]))\n",
    "            u_onehot.append(actions_onehot)\n",
    "            avail_u.append(avail_actions)\n",
    "            r.append([reward])\n",
    "            terminate.append([terminated])\n",
    "            padded.append([0.])\n",
    "            episode_reward += reward\n",
    "            step += 1\n",
    "            # ? epsilon decay update ?\n",
    "            #if self.args.epsilon_anneal_scale == 'step':\n",
    "            #    epsilon = epsilon - self.anneal_epsilon if epsilon > self.min_epsilon else epsilon\n",
    "        \n",
    "        # last obs\n",
    "        o.append(obs)\n",
    "        s.append(state)\n",
    "        o_next = o[1:]\n",
    "        s_next = s[1:]\n",
    "        o = o[:-1]\n",
    "        s = s[:-1]\n",
    "        \n",
    "        # get avail_action for last obs，because target_q needs avail_action in training\n",
    "        avail_actions = []\n",
    "        for agent_id in range(self.n_agents):\n",
    "            avail_action = self.env.get_avail_agent_actions(agent_id)\n",
    "            avail_actions.append(avail_action)\n",
    "        avail_u.append(avail_actions)\n",
    "        avail_u_next = avail_u[1:]\n",
    "        avail_u = avail_u[:-1]\n",
    "        \n",
    "        # if step < self.episode_limit，padding\n",
    "        for i in range(step, self.episode_limit):\n",
    "            o.append(np.zeros((self.n_agents, self.obs_shape)))\n",
    "            u.append(np.zeros([self.n_agents, 1]))\n",
    "            s.append(np.zeros(self.state_shape))\n",
    "            r.append([0.])\n",
    "            o_next.append(np.zeros((self.n_agents, self.obs_shape)))\n",
    "            s_next.append(np.zeros(self.state_shape))\n",
    "            u_onehot.append(np.zeros((self.n_agents, self.n_actions)))\n",
    "            avail_u.append(np.zeros((self.n_agents, self.n_actions)))\n",
    "            avail_u_next.append(np.zeros((self.n_agents, self.n_actions)))\n",
    "            padded.append([1.]) \n",
    "            terminate.append([1.])\n",
    "        episode = dict(o = o.copy(),\n",
    "                       s = s.copy(),\n",
    "                       u = u.copy(),\n",
    "                       r = r.copy(),\n",
    "                       avail_u = avail_u.copy(),\n",
    "                       o_next = o_next.copy(),\n",
    "                       s_next = s_next.copy(),\n",
    "                       avail_u_next=avail_u_next.copy(),\n",
    "                       u_onehot=u_onehot.copy(),\n",
    "                       padded=padded.copy(),\n",
    "                       terminated=terminate.copy()\n",
    "                      )\n",
    "        # add episode dim\n",
    "        for key in episode.keys():\n",
    "            episode[key] = np.array([episode[key]])\n",
    "        \n",
    "        return episode, episode_reward, win_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment: # wrapper for pommerman environment\n",
    "    def __init__(self, type='PommeFFACompetition-v0'):\n",
    "        self.agents = [QMixAgent(0)]\n",
    "        self.n_agents = len(self.agents)\n",
    "        self.opponents   = [agents.SimpleAgent()]\n",
    "        agent_list = self.agents + self.opponents\n",
    "        self.env = pommerman.make(type, agent_list)\n",
    "        self.env.reset()\n",
    "        self.n_actions = self.env.action_space.n\n",
    "    \n",
    "    def _featurize(self, obs):\n",
    "        \"\"\"Returns a tensor of size 11x11x18\"\"\"\n",
    "        # TODO: history of n moves?\n",
    "        board = obs['board']\n",
    "\n",
    "        # convert board items into bitmaps\n",
    "        maps = [board == i for i in range(10)]\n",
    "        maps.append(obs['bomb_blast_strength'])\n",
    "        maps.append(obs['bomb_life'])\n",
    "\n",
    "        # duplicate ammo, blast_strength and can_kick over entire map\n",
    "        maps.append(np.full(board.shape, obs['ammo']))\n",
    "        maps.append(np.full(board.shape, obs['blast_strength']))\n",
    "        maps.append(np.full(board.shape, obs['can_kick']))\n",
    "\n",
    "        # add my position as bitmap\n",
    "        position = np.zeros(board.shape)\n",
    "        position[obs['position']] = 1\n",
    "        maps.append(position)\n",
    "\n",
    "        # add teammate\n",
    "        if obs['teammate'] is not None:\n",
    "            maps.append(board == obs['teammate'].value)\n",
    "        else:\n",
    "            maps.append(np.zeros(board.shape))\n",
    "\n",
    "        # add enemies\n",
    "        enemies = [board == e.value for e in obs['enemies']]\n",
    "        maps.append(np.any(enemies, axis=0))\n",
    "\n",
    "        out = np.stack(maps, axis=2) \n",
    "        # transpose to CxHxW\n",
    "        return out.transpose((2, 0, 1))\n",
    "    \n",
    "    def get_agents(self):\n",
    "        return self.agents\n",
    "    \n",
    "    def get_state(self):\n",
    "        # returns state = np.array of size n_agents x 18x11x11\n",
    "        return np.stack(self.get_observations())\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "    \n",
    "    def get_observations(self):\n",
    "        \"Returns list of observations for our agents\"\n",
    "        obs = self.env.get_observations()\n",
    "        return [self._featurize(o) for o in obs[:self.n_agents]]\n",
    "    \n",
    "    def get_avail_agent_actions(self, agent_id):\n",
    "        \"\"\"return avail_action\n",
    "        \"\"\"\n",
    "        # TODO: iplement this correctly\n",
    "        return list(range(len(pommerman.constants.Action)))\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"return reward, terminated, info\n",
    "           info contains 'battle_won' if won\n",
    "        \"\"\"\n",
    "        observations = self.env.get_observations()\n",
    "        for idx, opponent in enumerate(self.opponents):\n",
    "            action = self.opponent.act(observations[self.n_agents+idx], self.env.action_space)\n",
    "            actions += action\n",
    "        _, reward, done, _ = env.step(actions)\n",
    "        info = {}\n",
    "        if done:\n",
    "            if reward == 1:\n",
    "                info = {'battle_won': True}\n",
    "            elif reward == -1:\n",
    "                info = {'battle_won': False}\n",
    "        return reward, done, info\n",
    "        \n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "class args:\n",
    "    buffer_size    = int(10e4)\n",
    "    min_train_size = 100\n",
    "    batch_size     = 64\n",
    "    gamma          = 0.9\n",
    "    lr             = 0.001\n",
    "    alpha          = 0.9\n",
    "    print_interval = 20\n",
    "    n_epochs       = 50\n",
    "    n_steps        = 200\n",
    "    verbose        = False\n",
    "    epsilon        = 1.0\n",
    "    eps_decay      = 1 - 10e-9\n",
    "    eps_min        = 0.05\n",
    "    episode_limit  = 100\n",
    "    n_actions      = len(pommerman.constants.Action)\n",
    "    state_shape    = (1, 18, 11, 11)\n",
    "    obs_shape      = (18, 11, 11)\n",
    "    device         = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-d7e5acca7f5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_agents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_agents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRolloutWorker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAgents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-393f053568ba>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQMix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Initialized Agents'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-f186b8fe1295>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#input_shape += self.n_agents  # reuse agent network for all agents (-> weight sharing)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_rnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# the agent network that produces Q_a(.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_rnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_qmix_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQMixNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# the mixer netwok Qtot = f(Q1, ..., Qn, state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RNN' is not defined"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "args.n_agents = env.n_agents\n",
    "row = RolloutWorker(env, Agents(args), args)\n",
    "row.generate_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner:\n",
    "    def __init__(self, env, args):\n",
    "        self.args = args\n",
    "        self.env = pommerman.make('PommeFFACompetition-v0', agent_list)\n",
    "        self.agents = Agents(env)\n",
    "        \n",
    "        self.rollout_worker = RolloutWorker()\n",
    "        \n",
    "        self.win_rates = []\n",
    "        self.episode_rewards = []\n",
    "    \n",
    "    def run(self, num):\n",
    "        train_steps = 0\n",
    "        for epoch in range(self.args.n_epochs):\n",
    "            print(f\"Run {num}, train epoch {epoch}\")\n",
    "            if epoch % self.args.evaluate_cycle == 0:\n",
    "                win_rate, episode_reward = self.evaluate()\n",
    "                self.win_rates.append(win_rate)\n",
    "                self.episode_rewards.append(episode_reward)\n",
    "                # self.plt(num)\n",
    "            \n",
    "            episodes = []\n",
    "            for episode_idx in range(self.args.n_episodes):\n",
    "                episode, _, _ = self.rollout_worker.generate_episode(episode_idx)\n",
    "                episodes.append(episode)\n",
    "            episode_batch = episodes.pop(0)\n",
    "            for episode in episodes:\n",
    "                for key in episode_batch.keys(): # TODO: what does this do?\n",
    "                    episode_batch[key] = np.concatenate((episode_batch[key], episode[key]),\n",
    "                                                        axis=0)\n",
    "                self.buffer.store_episode(episode_batch)\n",
    "                for train_step in range(self.args.train_steps):\n",
    "                    # train_steps: to indicate when to sync eval and target models\n",
    "                    mini_batch = self.buffer.sample(min(self.buffer.current_size, self.args.batch_size))\n",
    "                    self.agents.train(mini_batch, train_steps)\n",
    "                    train_steps += 1\n",
    "    \n",
    "    def evaluate(self):\n",
    "        win_number = 0\n",
    "        episode_rewards = 0\n",
    "        for epoch in range(self.args.evaluate_epochs):\n",
    "            _, episode_reward, win_tag = self.rollout_worker.generate_episode(epoch, evaluate=True)\n",
    "            episode_rewards += episode_reward\n",
    "            if win_tag:\n",
    "                win_number += 1\n",
    "        return (win_number / self.args.evaluate_epochs, \\\n",
    "               episode_rewards / self.args.evaluate_epochs)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'env' and 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7ecfa73719aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'env' and 'args'"
     ]
    }
   ],
   "source": [
    "runner = Runner()\n",
    "runner.run(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[10, 7]\n[70]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "a = [2, 5, 7]\n",
    "def cumproduct(seq):\n",
    "    if len(seq) == 1: return seq[0]\n",
    "    else:\n",
    "        x = seq.pop(0)\n",
    "        seq[0] *= x\n",
    "        return cumproduct(seq)\n",
    "cumproduct(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pommerman.agents import SimpleAgent\n",
    "agent_list = [SimpleAgent() for _ in range(2)]\n",
    "env = pommerman.make('PommeFFACompetition-v0', agent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Object `env.set_training_agent` not found.\n"
     ]
    }
   ],
   "source": [
    "env.set_training_agent??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "import torch\n",
    "torch.empty((2,3,4)).shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.6.8 64-bit",
   "display_name": "Python 3.6.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0722e2e244714dc09bba630a310d2913cff8c9673941d67bca443422bbd50b9b"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}